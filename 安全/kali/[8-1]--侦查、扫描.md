
# 侦查、扫描

* 侦查
    - httrack
* 扫描
    - nikto
    - vega
    - skipfish
    - w3af
    - arachni
    - owasp-zap

## 侦查

* httrack
```shell 
    httrack http://topspeedsnail.com -O /tmp/topspeedsnail
```

## 扫描

### nikto
```shell 
    配置文件 /etc/nikto.conf
    nikto -list-plugins
        -host
        -port
        -ssl
        -output
        -vhost <域名>
    nikto -update
    nikto -host http://1.1.1.1 -port 8080
    nikto -host host.txt
    nmap -p80 192.168.1.0/24 -oG - |  nikto -host -
    nikto -host 192.168.1.1 -ssl -port 443
    nikto -host 192.168.1.1 -useproxy http://1.1.1.1:8081
```

### vega
* 描述
> JAVA编写的开源Web扫描器  
> 扫描模式  
> 代理模式  
> 爬站、处理表单、注入测试   
> 支持SSL  

* 网址
> 官方（https://subgraph.com/vega/）  
> kali（https://tools.kali.org/web-applications/veg）)

* 功能
1. 自动抓取工具和漏洞扫描程序
1. 一致的用户界面
1. 网站爬虫
1. 截取代理
1. SSL MITM
1. 内容分析
1. 通过强大的Javascript模块API实现可扩展性
1. 可定制的警报
1. 数据库和共享数据模型


* 使用基本流程
1. 先使用代理模式进行手工扫描
1. 手动访问网站内的每一个连接并测试每一个表单
1. 在使用扫描模式对扫描结果进行自动化测试
1. 在使用代理模式进行截断代理


### skipfish

* 简介
> Skipfish是一款主动的Web应用程序安全侦察工具。它通过执行递归爬取和基于字典的探测来为目标站点准备交互式站点地图。最终的地图然后用来自许多活动（但希望是不中断的）安全检查的输出来注释。该工具生成的最终报告旨在作为专业Web应用程序安全评估的基础。

* 参数
``` 
Authentication and access options:
 	
 	  -A user:pass      - use specified HTTP authentication credentials
 	  -F host=IP        - pretend that 'host' resolves to 'IP'
 	  -C name=val       - append a custom cookie to all requests
 	  -H name=val       - append a custom HTTP header to all requests
 	  -b (i|f|p)        - use headers consistent with MSIE / Firefox / iPhone
 	  -N                - do not accept any new cookies
 	  --auth-form url   - form authentication URL
 	  --auth-user user  - form authentication user
 	  --auth-pass pass  - form authentication password
 	  --auth-verify-url -  URL for in-session detection
 	
 	Crawl scope options:
 	
 	  -d max_depth     - maximum crawl tree depth (16)
 	  -c max_child     - maximum children to index per node (512)
 	  -x max_desc      - maximum descendants to index per branch (8192)
 	  -r r_limit       - max total number of requests to send (100000000)
 	  -p crawl%        - node and link crawl probability (100%)
 	  -q hex           - repeat probabilistic scan with given seed
 	  -I string        - only follow URLs matching 'string'
 	  -X string        - exclude URLs matching 'string'
 	  -K string        - do not fuzz parameters named 'string'
 	  -D domain        - crawl cross-site links to another domain
 	  -B domain        - trust, but do not crawl, another domain
 	  -Z               - do not descend into 5xx locations
 	  -O               - do not submit any forms
 	  -P               - do not parse HTML, etc, to find new links
 	
 	Reporting options:
 	
 	  -o dir          - write output to specified directory (required)
 	  -M              - log warnings about mixed content / non-SSL passwords
 	  -E              - log all HTTP/1.0 / HTTP/1.1 caching intent mismatches
 	  -U              - log all external URLs and e-mails seen
 	  -Q              - completely suppress duplicate nodes in reports
 	  -u              - be quiet, disable realtime progress stats
 	  -v              - enable runtime logging (to stderr)
 	
 	Dictionary management options:
 	
 	  -W wordlist     - use a specified read-write wordlist (required)
 	  -S wordlist     - load a supplemental read-only wordlist
 	  -L              - do not auto-learn new keywords for the site
 	  -Y              - do not fuzz extensions in directory brute-force
 	  -R age          - purge words hit more than 'age' scans ago
 	  -T name=val     - add new form auto-fill rule
 	  -G max_guess    - maximum number of keyword guesses to keep (256)
 	
 	  -z sigfile      - load signatures from this file
 	
 	Performance settings:
 	
 	  -g max_conn     - max simultaneous TCP connections, global (40)
 	  -m host_conn    - max simultaneous connections, per target IP (10)
 	  -f max_fail     - max number of consecutive HTTP errors (100)
 	  -t req_tmout    - total request response timeout (20 s)
 	  -w rw_tmout     - individual network I/O timeout (10 s)
 	  -i idle_tmout   - timeout on idle HTTP connections (10 s)
 	  -s s_limit      - response size limit (400000 B)
 	  -e              - do not keep binary responses for reporting
 	
 	Other settings:
 	
 	  -l max_req      - max requests per second (0.000000)
 	  -k duration     - stop scanning after the given duration h:m:s
 	  --config file   - load the specified configuration file
 	
```

#### 实例
* 只扫描包含 ‘字符串’ 的URL
``` 
skipfish -o test1 -I /dvwa/ http://172.16.10.133/dvwa/

```
* 指定目标 IP 列表
``` 
vim iplist.txt
skipfish -o test1 @iplist.txt
```

* 指定扫描内容和扫描字典
``` 
# 默认扫描使用的字典
dpkg -L skipfish | grep wl

# 指定字典 （-S）
skipfish -o test1 -I /dvwa/ -S /usr/share/skipfish/dictionaries/minimal.wl http://172.16.10.133/dvwa/

# 将目标网站特有的特征漏洞代码存到文件 （-W
skipfish -o test1 -I /dvwa/ -S /usr/share/skipfish/dictionaries/minimal.wl -W abc.wl http://172.16.10.133/dvwa/
```

* 指定最大连接数
``` 
skipfish -o test1 -l 2000 -S /usr/share/skipfish/dictionaries/minimal.wl http://172.16.10.133/dvwa/

skipfish -o test1 -l 10 -S /usr/share/skipfish/dictionaries/minimal.wl http://172.16.10.133/dvwa/
```

* 指定并发连接数
``` 
skipfish -o test1 -m 200 -S /usr/share/skipfish/dictionaries/minimal.wl http://172.16.10.133/dvwa/
```

* 身份认证
``` 
skipfish -A user:pass -o test http://1.1.1.1
skipfish -C “name=val” -o test http://1.1.1.1
Username / Password
```

* 用户名密码验证
``` 
 skipfish -A admin:password -I /dvwa/ -o test1 http://172.16.10.133/dvwa/
 
 skipfish -C "PHPSESSID=a5b1d5b679e934f24bf6ae172dfbf8e0" -C "security=low" -X logout.php -I /dvwa/ -o test1 http://172.16.10.133/dvwa/
```

* 提交用户名密码表单
``` 
skipfish -o test1 --auth-form http://172.16.10.133/dvwa/login.php --auth-form-target http://172.16.10.133/dvwa/login.php --auth-user-field username --auth-user admin --auth-pass-field password --auth-pass password --auth-verify-url http://172.16.10.133/dvwa/index.php -I /dvwa/ -X logout.php http://172.16.10.133/dvwa/
```

